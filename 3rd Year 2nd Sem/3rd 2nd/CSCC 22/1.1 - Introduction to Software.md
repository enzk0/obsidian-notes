---
Course: CSCC 22
Year Level: 3rd
Semester: 2nd
---
---

### Test Cases
---
1. ***Technical Case***
	- Affects the *reputation* of the Developer
	- Competent developers are *not* *infallible* (incapable making mistakes)
	- Implications of requirements are not always foreseeable
	- Has bugs that causes application failures
2. ***Business Case***
	- Doesn't compromise *quality*
	- Users might not use your product because it is faulty
	- Most **Costly** due to post-release debugging. Developing right *back from the start*, *another iteration*
	- Affects the *reputation* of the Company
3. ***Professional Case***
	- Testing is a challenging and rewarding task
	- Systematic Testing is Good Testing
	- *Developer confidence and Credibility develops* due to quality work

> **systematic testing**, a structured and thorough testing plan that is in the context of the project.

4. ***Economics Case***
	- defects get introduced in every phase of SDLC
	- *Phase Containment of Errors* - detecting errors on its introduction, gets stronger if not solved within its own phase

> *When meeting the client:*
	a. provide a solution
	b. the client should be the one to talk
	c. provide questions to ask the client about what they want, their challenges, their needs
	d. just listen
	 *"The wrong thing to do is ask the budget first, do it at later times"*
	 - aim to compute cost after going through the specifics and context of the project
	*Deliverables of **Requirements Analysis**:*
		a. SRS: System Requirements Specifications
		b. project scopes and limitations
	Verification and validation early on
	Quality assurance is on testing but should be done pre and post
		- To Improve Quality
		- For Verification and Validation
		- For Reliability Estimation

5. ***For Quality Improvement***
	- bugs can be very severe and causes huge losses
	- so monitoring the health of your system is important
	- *test* doesn't end when you release the software
6. ***For Verification and Validation***
	- *testing* serves as *metrics*
	- used as a tool in the V&V process
	- comparing quality among different products under the same specification
![[Pasted image 20250220212636.png]]
7. ***For Reliability Estimation***
	- *Software reliability* has important relationships with many aspects of software, including the *structure* and the *amount of testing done* to the software
	- based on operational profile, testing can serve as a statistical sampling method to gain failure data for reliability estimation

### Epic Software Failures
---
1. **SolarWinds**
	- FireEye, a cybersecurity company, and Microsoft, affected by the SolarWinds vulnerability
2. **Yahoo** **Breach**
	- credentials exploited
3. **Nest** **Thermostat** **Freeze**
	- uncomfortable and uncontrollable temperature, due to firmware update
	- *"make sure to do testing in all scenarios, across different platforms"*
4. **HSBC's Payment Glitch**
	- failure of electronic payment system

### Why Software Failures Happen?
---
- *software testing constitutes* ***40% of overall effort*** and ***25% budget of the overall budget***
- due to *lack of time*, it leads to *inadequate testing*. Defects are left behind, only to be found out by the user
- Results in loss of client trust, to the company, and to the leader
- software is a ***ubiquitous product***, *90% of people use software*
- loss of smaller companies, due to not enough attention to software quality and conducting the right amount of testing
- new technologies, new test cases

## Who Should do Testing
---
- ***testing starts from the very beginning***
- testing is a *team effort*, everyone's responsibility 

> **developers** test by unit testing
> **designers** do design review

### How much should we test
---
- there is no 100% testing, as testing is not 100% exhaustive
- there's an infinite number of test cases

**Prioritize:**
- core components

#### Selection of Good Test Cases
---
Designing a good test case is a *complex art*
- different types of tests need different classes of information
- test cases within a test suite will not be good
- test cases come from use cases
- people create test cases according to certain testing styles
	- *domain testing* or *risk-based testing*
	- *good domain tests* are **different** from *good risk-based tests*

*"A test idea is a brief statement of something that should be tested."*
*"The best test cases are the ones that finds bugs."*

> **test suite**, collection of test cases
> **test case**, a question that you ask of the program

#### When to stop testing
---
- testing is *potentially endless*

but when to stop?
- testing is a **trade-off** between *budget*, *time*, and *quality*. Driven by profit models
- project manager exists to ensure that member works on the timeline

***pessimistic approach***, is to stop testing whenever some or any of the allocated resources are depleted

***optimistic stopping rule***, is to stop testing when either reliability meets the requirement, or the benefit from continuing testing cannot justify the testing cost