
### ML + FL Studies
---

#### TRAINING NEURAL NETWORKS AS RECOGNIZERS OF FORMAL LANGUAGES
---
Authors: Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell


# Not yet in ANARA
---
- [[35] Liu, L., Liu, P., Ga, L. and Ai, J. 2021. Advances in Applications of Molecular Logic Gates. ACS Omega. 6, 45 (Nov. 2021), 30189–30204. DOI:https://doi.org/10.1021/acsomega.1c02912.](https://www.zotero.org/google-docs/?broken=daupXV)
- **[Logic Gates: Symbol, Types, Truth Table and Boolean Expression: https://testbook.com/digital-electronics/logic-gates. Accessed: 2025-07-07.](https://www.zotero.org/google-docs/?broken=s0ZQea)**
- **[McCulloch, W. and Pitts, W. 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics. 5, (1943), 115–133.](https://www.zotero.org/google-docs/?broken=3bbnsm)**
- **[Natarajan, D. 2020. Logic Gates. Fundamentals of Digital Electronics. Springer International Publishing. 13–28.](https://www.zotero.org/google-docs/?broken=HLioqy)**
- **[[42] Nuñez-Andrade, E., Vidal-Daza, I., Ryan, J.W., Gómez-Bombarelli, R. and Martin-Martinez, F.J. 2025. Embedded machine-readable molecular representation for resource-efficient deep learning applications. Digital Discovery. 4, 3 (Mar. 2025), 776–789. DOI:https://doi.org/10.1039/D4DD00230J.](https://www.zotero.org/google-docs/?broken=msAffv.
- **[[45] Qin, H., Gong, R., Liu, X., Bai, X., Song, J. and Sebe, N. 2020. Binary Neural Networks: A Survey. Pattern Recognition. 105, (Sept. 2020), 107281. DOI:https://doi.org/10.1016/j.patcog.2020.107281.](https://www.zotero.org/google-docs/?broken=6nG9w9)**
- **[[46] Rumelhart, D.E., Hinton, G.E. and Williams, R. 1986. Learning representation by backpropagating errors. Nature. 323, 6088 (1986), 533–536.](https://www.zotero.org/google-docs/?broken=DF4MnF)**
- **[[48] Srinivas, S., Subramanya, A. and Babu, R.V. 2017. Training Sparse Neural Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (Honolulu, HI, USA, July 2017), 455–462.](https://www.zotero.org/google-docs/?broken=m2MZvN)**
- **[[50] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I. 2023. Attention Is All You Need. arXiv.](https://www.zotero.org/google-docs/?broken=Bw6L50)**
- **[[51] Waskom, M. 2021. seaborn: statistical data visualization. Journal of Open Source Software. 6, 60 (Apr. 2021), 3021. DOI:https://doi.org/10.21105/joss.03021.](https://www.zotero.org/google-docs/?broken=duYoCK)**
- **[[52] Yousefi, S., Plesner, A., Aczel, T. and Wattenhofer, R. 2025. Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks. arXiv.](https://www.zotero.org/google-docs/?broken=dxKkmv)**
- **[[53] Yuan, C. and Agaian, S.S. 2023. A comprehensive review of Binary Neural Network. Artificial Intelligence Review. 56, 11 (Nov. 2023), 12949–13013. DOI:https://doi.org/10.1007/s10462-023-10464-w.](https://www.zotero.org/google-docs/?broken=sjfapr)**
- **[[54] Zhang, Y., Barzilay, R. and Jaakkola, T. 2017. Aspect-augmented Adversarial Networks for Domain                     Adaptation. Transactions of the Association for Computational Linguistics. 5, (Dec. 2017), 515–528. DOI:https://doi.org/10.1162/tacl_a_00077.](https://www.zotero.org/google-docs/?broken=qnJgLP)**


Early studies on neural networks as formal language recognizers focused on training the models directly under the accept/reject paradigm, which mimics the behavior of a classical automata. A significant line of work involves neural models augmented with differentiable memory structures. For instance, the Neural Network Pushdown Automaton (NNPDA) introduced by Sun et. al. demonstrated that a recurrent controller with a continuous stack can approximate context-free languages such as _aⁿbⁿ_ and Dyck languages [giles]. This demonstrated that neural networks can actually mirror pushdown-style computation when given appropriate inductive biases. Similar investigations by Rodriguez and others found that standard recurrent architectures like LSTMs can sometimes learn simple context-free patterns but often struggle to generalize to longer strings beyond the training range [rodriguez].