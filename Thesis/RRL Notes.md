
### ML + FL Studies
---

#### TRAINING NEURAL NETWORKS AS RECOGNIZERS OF FORMAL LANGUAGES
---
Authors: Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell


# Not yet in ANARA
---
- [[35] Liu, L., Liu, P., Ga, L. and Ai, J. 2021. Advances in Applications of Molecular Logic Gates. ACS Omega. 6, 45 (Nov. 2021), 30189–30204. DOI:https://doi.org/10.1021/acsomega.1c02912.](https://www.zotero.org/google-docs/?broken=daupXV)
- **[Logic Gates: Symbol, Types, Truth Table and Boolean Expression: https://testbook.com/digital-electronics/logic-gates. Accessed: 2025-07-07.](https://www.zotero.org/google-docs/?broken=s0ZQea)**
- **[McCulloch, W. and Pitts, W. 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics. 5, (1943), 115–133.](https://www.zotero.org/google-docs/?broken=3bbnsm)**
- **[Natarajan, D. 2020. Logic Gates. Fundamentals of Digital Electronics. Springer International Publishing. 13–28.](https://www.zotero.org/google-docs/?broken=HLioqy)**
- **[[42] Nuñez-Andrade, E., Vidal-Daza, I., Ryan, J.W., Gómez-Bombarelli, R. and Martin-Martinez, F.J. 2025. Embedded machine-readable molecular representation for resource-efficient deep learning applications. Digital Discovery. 4, 3 (Mar. 2025), 776–789. DOI:https://doi.org/10.1039/D4DD00230J.](https://www.zotero.org/google-docs/?broken=msAffv.
- **[[45] Qin, H., Gong, R., Liu, X., Bai, X., Song, J. and Sebe, N. 2020. Binary Neural Networks: A Survey. Pattern Recognition. 105, (Sept. 2020), 107281. DOI:https://doi.org/10.1016/j.patcog.2020.107281.](https://www.zotero.org/google-docs/?broken=6nG9w9)**
- **[[46] Rumelhart, D.E., Hinton, G.E. and Williams, R. 1986. Learning representation by backpropagating errors. Nature. 323, 6088 (1986), 533–536.](https://www.zotero.org/google-docs/?broken=DF4MnF)**
- **[[48] Srinivas, S., Subramanya, A. and Babu, R.V. 2017. Training Sparse Neural Networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (Honolulu, HI, USA, July 2017), 455–462.](https://www.zotero.org/google-docs/?broken=m2MZvN)**
- **[[50] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I. 2023. Attention Is All You Need. arXiv.](https://www.zotero.org/google-docs/?broken=Bw6L50)**
- **[[51] Waskom, M. 2021. seaborn: statistical data visualization. Journal of Open Source Software. 6, 60 (Apr. 2021), 3021. DOI:https://doi.org/10.21105/joss.03021.](https://www.zotero.org/google-docs/?broken=duYoCK)**
- **[[52] Yousefi, S., Plesner, A., Aczel, T. and Wattenhofer, R. 2025. Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks. arXiv.](https://www.zotero.org/google-docs/?broken=dxKkmv)**
- **[[53] Yuan, C. and Agaian, S.S. 2023. A comprehensive review of Binary Neural Network. Artificial Intelligence Review. 56, 11 (Nov. 2023), 12949–13013. DOI:https://doi.org/10.1007/s10462-023-10464-w.](https://www.zotero.org/google-docs/?broken=sjfapr)**
- **[[54] Zhang, Y., Barzilay, R. and Jaakkola, T. 2017. Aspect-augmented Adversarial Networks for Domain                     Adaptation. Transactions of the Association for Computational Linguistics. 5, (Dec. 2017), 515–528. DOI:https://doi.org/10.1162/tacl_a_00077.](https://www.zotero.org/google-docs/?broken=qnJgLP)**


Early studies on neural networks as formal language recognizers focused on training the models directly under the accept/reject paradigm, which mimics the behavior of a classical automata. A significant line of work involves neural models augmented with differentiable memory structures. For instance, the Neural Network Pushdown Automaton (NNPDA) introduced by Sun et. al. demonstrated that a recurrent controller with a continuous stack can approximate context-free languages such as _aⁿbⁿ_ and Dyck languages [giles]. This demonstrated that neural networks can actually mirror pushdown-style computation when given appropriate inductive biases such as an external memory stack. Similar investigations by Rodriguez et. al. found that the standard recurrent architectures like SRNs can sometimes learn simple context-free patterns but usually struggles to generalize to longer strings beyond the training range [rodriguez].

As the field evolved, researchers developed more expressive memory-augmented architectures to improve recognition capability. DuSell and Chiang proposed nondeterministic stack-augmented recurrent models capable of tracking multiple stack configurations in parallel, allowing them to learn more complex context-free languages than classical RNNs [dussell]. Subsequent work further extended these ideas by integrating differentiable nondeterministic stacks into both RNNs and Transformers, highlighting the potential and limitations of embedding symbolic operations directly into neural architecture [dussell2023]. Complementary interpretability studies, such as Merrill et al.’s analysis of neural stack models, showed that these networks often develop internal representations that mirror hierarchical syntactic structure [merrill].

More recent studies examined mainstream architectures—LSTMs, GRUs, and Transformers—on recognition tasks as a way to characterize their expressivity and inductive biases. Bhattamishra et al. showed that while Transformers can fit formal languages during training, they fail to reliably generalize to longer sequences or deeper hierarchical structures, even for relatively simple context-free languages like Dyck-1 [bhattamishra]. Other work demonstrated that LSTMs can exhibit limited algorithmic ability, such as performing approximate counting behaviors, but these abilities tend to be brittle and sensitive to training distribution [suzgun]. Collectively, these findings indicate that modern neural architectures possess the expressive capacity to approximate formal languages but often rely on superficial statistical cues rather than learning the underlying generative rules.

Despite the breadth of prior work, evaluations of neural networks as formal recognizers remained inconsistent. Each study relied on its own dataset, negative sampling strategy, and generalization protocol, frequently using randomly generated negative examples that reward shallow heuristics rather than rule learning. These methodological inconsistencies made it difficult to compare architectural performance or draw firm conclusions about neural networks’ ability to implement formal recognition. This gap directly motivated the development of standardized recognition benchmarks such as the FLaRe dataset, discussed in the following section.