
***Highlight changes and send new paper to Sir***
### Chapter 3 Changes
---
Neural networks—especially recurrent variants—have long been evaluated as **formal‑language recognizers**, i.e., as automata that must decide whether a given string belongs to a target language.  Theoretical analyses showed that standard RNNs are Turing‑complete in principle, yet empirical training with gradient‑based methods often restricts them to recognizing only simple regular or mildly context‑free patterns, exposing a gap between expressive power and learnability.  In practice, researchers have traditionally closed this gap by scaling models (more layers, larger hidden sizes, more data) to improve benchmark scores across vision, language and reasoning tasks [[26]].  This scaling, however, inflates computational cost and energy consumption [[12]].  Recent efficiency‑oriented proposals—low‑precision arithmetic [[9]] & [[18]], binary or ternary activations/weights [[36]], and extreme sparsity that pushes many parameters to zero [[23]]—reduce FLOPs but still rely on conventional floating‑point multiplications and divisions, which remain a bottleneck at scale [[21]].  Moreover, these compressed models tend to lose interpretability, making it difficult to trace the reasoning behind a recognition decision and requiring additional visualization tools to inspect learned representations [[28]] & [[20]].

To overcome both the algorithmic limitations of standard recurrent networks and the opacity introduced by aggressive compression, **Recurrent Deep Differentiable Logic Gate Networks (RDDLGN)** were introduced.  RDDLGN augments recurrence with differentiable Boolean logic gates, providing a principled way to perform logical operations within a trainable recurrent core.  This logic‑driven recurrence yields models that are intrinsically more aligned with the rule‑based nature of formal languages, offering improved efficiency, interpretability, and competitive performance on sequence tasks such as WMT‑14 English‑German translation [[5]].  The architecture builds directly on the original Deep Differentiable Logic Gate Network (DDLGN) framework and extends it with recurrent dynamics, bridging classic RNN/LSTM/GRU designs with explicit logical computation [[34]].  As a result, RDDLGN presents a promising alternative for formal‑language recognition, combining the expressive power of recurrent models with the transparency and computational benefits of differentiable logic.









Neural networks and its various architectures have shown great performance in many benchmarks across multiple modalities including computer vision, natural language processing, and reasoning tasks. The typical paradigm in improving the performance metrics of these deep learning architectures involve making the models bigger —in terms of parameters, layers, and hidden layers— while also training them with more data [26] (Kaplan). However, this makes models more computationally expensive and therefore, requires more energy and resources to build [12] (Desislavov).

In recent years, new architectures have been introduced that aim to make these models as fast and as efficient as possible. These include reducing the numerical precision of the network’s calculations [9, 18] (Choi)(Gupta), limiting either the activation values or parameters to binaries [36](Qin), and completely pushing its parameters to zero [23](Hoefler). These models were able to achieve decent performance while being computationally cheaper during training and inference. However, these implementations still rely on arithmetic operations —such as floating point divisions and multiplications— during training and inference, which when scaled to a larger number of inputs, becomes computationally expensive [21](Hastie). To make matters worse, since these techniques “take away” from the model, be it precision or complexity, the resulting models where such techniques were applied often performed worse than their unmodified counterparts. Additionally, these models are also not interpretable by default due to their lack of transparency, making it harder to understand the reasoning behind the model’s decisions [28](Li). More than that, it also requires various techniques and methodologies to visualize its learned representations [20](Harlev).

In response to these persistent issues, a new and novel architecture, RRDLGN, was made by Buhrer et al. The research conducted by Buhrer et al. was the first published paper that implemented recurrence in the DLGN model and allowed differential logic-gate networks to effectively handle sequential tasks such as machine translation [5](Buhrer, RDDLGN). Their work is inspired by the original DDLGN research [34](Petersen et al., 2022, 2024) and classic recurrent designs (RNN, LSTM, GRU), extending them with differentiable boolean logic to achieve efficient, interpretable, and performant recurrent neural networks. Consequently, RDDLGN is considered to be a high-performance neural network architecture that  achieves performance comparable to GRU baselines on WMT '14 English-German translation. Because of this, RDDLGN, as a neural network architecture, exhibits the potential to become an alternative to traditional recurrent neural network architectures and opens new research directions and real-world applications [5](Buhrer, RDDLGN).
